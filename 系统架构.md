好的，没问题。由于文本环境无法直接渲染Mermaid图，我为你提供一个详细的文字版描述，你可以根据这个描述来绘制你的系统总体架构图。

**系统总体架构图 - 文字描述**

**核心组件与层次：**

1.  **用户端 (Frontend - Client Tier)**
    *   **组件：** 用户浏览器中运行的React应用程序。
    *   **主要功能：** 用户界面展示、用户输入处理、与后端API交互。
    *   **交互方式：** 通过HTTP/HTTPS协议向后端服务发送RESTful API请求。

2.  **应用层/后端服务 (Backend - Application Tier - Node.js/Express.js)**
    *   **组件：** Express.js Web服务器。
    *   **主要功能：**
        *   接收并处理前端的API请求。
        *   执行业务逻辑（如参数校验、数据格式化、缓存管理）。
        *   与数据库交互（通过MongoDB驱动程序，但本项目中主要是通过Python脚本间接操作）。
        *   作为“任务调度器”或“协调者”，调用Python服务。
    *   **内部模块/路由：**
        *   `routes/cookie.js` (处理Cookie相关请求)
        *   `routes/listSearch.js` (处理列表搜索请求，包含缓存逻辑)
        *   `routes/analysis.js` (处理数据查询和情感分析请求，包含CSV文件生成)
    *   **交互方式：**
        *   响应前端的HTTP/HTTPS请求。
        *   通过Node.js的`child_process`模块（具体为`spawn`）启动Python桥接脚本作为子进程。
        *   通过子进程的`stdin`向Python脚本发送JSON格式的参数。
        *   通过子进程的`stdout`接收Python脚本返回的JSON格式结果。
        *   通过子进程的`stderr`捕获Python脚本的错误/日志信息。
        *   (可选，如果后端直接操作) 通过MongoDB Node.js Driver与MongoDB数据库通信。

3.  **Python服务层 (Python Services - Microservices/Scripts Tier)**
    *   **组件：** 一系列独立的Python脚本，被Node.js后端调用。
    *   **主要功能：** 执行计算密集型或特定库依赖的任务（爬虫、机器学习）。
    *   **具体脚本：**
        *   **`cookieBridge.py`:**
            *   职责：处理微博二维码生成、登录状态检查。
            *   交互：与外部“微博认证服务”交互，读取/更新本地`config.toml`文件（存储Cookie）。
        *   **`listSearchBridge.py`:**
            *   职责：协调微博列表搜索和数据爬取。
            *   交互：调用“微博爬虫核心模块”，并将数据直接写入“MongoDB数据库”。
        *   **`analysisBridge.py`:**
            *   职责：协调MongoDB数据查询和情感分析任务。
            *   交互：
                *   通过MongoDB Python Driver从“MongoDB数据库”查询数据。
                *   调用`sentiment.py`模块执行情感分析。
                *   (注意：CSV文件是在Node.js后端根据此脚本返回的数据生成的，此脚本本身不直接写CSV给Node.js，而是返回处理好的数据列表)。
        *   **`sentiment.py` (情感分析核心模块):**
            *   职责：加载BERT模型和自定义分类头，对输入的文本数据进行情感预测和结果聚合。
            *   依赖：本地存储的BERT模型文件和分类头权重文件。

4.  **数据存储层 (Data Tier)**
    *   **组件：** MongoDB数据库。
    *   **主要功能：** 持久化存储社交媒体数据。
    *   **存储内容：**
        *   微博帖子数据（由`listSearchBridge.py`通过爬虫核心模块写入）。
        *   微博评论数据（如果实现了评论爬取，同样由爬虫模块写入）。
        *   (间接) Cookie信息存储在Python爬虫项目根目录下的`config.toml`文件中。

5.  **外部服务与资源 (External Services & Resources)**
    *   **组件：**
        *   微博平台 (API接口 / HTML页面)：数据采集的目标。
        *   微博认证服务：二维码登录流程依赖的微博官方服务。
        *   本地模型文件：BERT预训练模型、自定义分类头权重。

**数据流与连接线（用箭头表示方向）：**

*   **用户浏览器 (React App)** `--- HTTP/HTTPS (RESTful API) --->` **Express.js 后端服务**
*   **Express.js 后端服务** `--- child_process (stdin/stdout JSON) --->` **`cookieBridge.py`**
*   **Express.js 后端服务** `--- child_process (stdin/stdout JSON) --->` **`listSearchBridge.py`**
*   **Express.js 后端服务** `--- child_process (stdin/stdout JSON) --->` **`analysisBridge.py`**
*   **`cookieBridge.py`** `--- HTTP (与微博服务器) --->` **微博认证服务**
*   **`cookieBridge.py`** `--- 文件读/写 --->` **`config.toml` (本地文件)**
*   **`listSearchBridge.py`** `--- 调用 --->` **微博爬虫核心模块 (pack, request, parse等)**
*   **微博爬虫核心模块** `--- HTTP (与微博服务器) --->` **微博平台 (API/HTML)**
*   **微博爬虫核心模块** `--- MongoDB Python Driver --->` **MongoDB 数据库**
*   **`analysisBridge.py`** `--- MongoDB Python Driver --->` **MongoDB 数据库** (用于`execute_query`动作)
*   **`analysisBridge.py`** `--- 调用 --->` **`sentiment.py` (情感分析核心模块)** (用于`analyze_sentiment_from_csv`动作，数据来自Node.js生成的CSV)
*   **`sentiment.py`** `--- 文件读取 --->` **本地模型文件 (BERT, 分类头权重)**

**图示建议：**

*   使用不同颜色的框或泳道来区分不同的层次（用户端、后端服务、Python服务、数据存储、外部服务）。
*   清晰地标出每个组件的名称和主要技术（如React, Express, Python, MongoDB）。
*   用带箭头的线表示数据流或调用关系，并在箭头上标注交互方式（如RESTful API, child_process, DB Driver）。
*   对于Python服务层，可以将其中的桥接脚本和核心功能模块（如爬虫核心、sentiment.py）进行区分展示，表明桥接脚本是对核心功能的封装和调用。

这个文字描述应该足够详细，可以帮助你绘制出一幅清晰准确的系统架构图了。